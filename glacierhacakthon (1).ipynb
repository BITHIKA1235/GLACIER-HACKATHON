{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1143211-73b1-4064-832a-6bf7aedbfb7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'GLACIER HACKATHON\\\\train\\\\Train\\\\Band1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m band_files_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m band_dir \u001b[38;5;129;01min\u001b[39;00m band_dirs:\n\u001b[1;32m---> 35\u001b[0m     files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(band_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.img\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[0;32m     36\u001b[0m     files\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m     37\u001b[0m     band_files_list\u001b[38;5;241m.\u001b[39mappend(files)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'GLACIER HACKATHON\\\\train\\\\Train\\\\Band1'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import re\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load dataset with proper validation split\n",
    "# -------------------------\n",
    "base_dir = os.path.join(\"GLACIER HACKATHON\", \"train\", \"Train\")\n",
    "band_dirs = [\n",
    "    os.path.join(base_dir, \"Band1\"),\n",
    "    os.path.join(base_dir, \"Band2\"),\n",
    "    os.path.join(base_dir, \"Band3\"),\n",
    "    os.path.join(base_dir, \"Band4\"),\n",
    "    os.path.join(base_dir, \"Band5\")\n",
    "]\n",
    "label_dir = os.path.join(base_dir, \"label\")\n",
    "\n",
    "# Collect band and label files\n",
    "band_files_list = []\n",
    "for band_dir in band_dirs:\n",
    "    files = [f for f in os.listdir(band_dir) if f.lower().endswith(('.tif', '.img'))]\n",
    "    files.sort()\n",
    "    band_files_list.append(files)\n",
    "\n",
    "label_files = [f for f in os.listdir(label_dir) if f.lower().endswith(('.tif', '.img'))]\n",
    "label_files.sort()\n",
    "\n",
    "# -------------------------\n",
    "# 2. Key Improvement: Split by glacier region (geographical split)\n",
    "# -------------------------\n",
    "def extract_region_id(filename):\n",
    "    \"\"\"Extract region identifier from filename\"\"\"\n",
    "    patterns = [\n",
    "        r'(Region[A-Z]+)_glacier',\n",
    "        r'([A-Za-z]+)_glacier',\n",
    "        r'(region[A-Z]+)_',\n",
    "        r'([A-Za-z0-9]+)_glacier'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "region_files = {}\n",
    "for label_file in label_files:\n",
    "    region_id = extract_region_id(label_file)\n",
    "    if region_id not in region_files:\n",
    "        region_files[region_id] = []\n",
    "    region_files[region_id].append(label_file)\n",
    "\n",
    "regions = list(region_files.keys())\n",
    "print(f\"Available regions: {regions}\")\n",
    "\n",
    "if len(regions) < 2:\n",
    "    random.shuffle(label_files)\n",
    "    split_idx = int(0.8 * len(label_files))\n",
    "    train_label_files = label_files[:split_idx]\n",
    "    val_label_files = label_files[split_idx:]\n",
    "    print(\"Warning: Only one region found, using random split\")\n",
    "else:\n",
    "    val_region = regions[0]\n",
    "    train_regions = regions[1:]\n",
    "    train_label_files = []\n",
    "    for region in train_regions:\n",
    "        train_label_files.extend(region_files[region])\n",
    "    val_label_files = region_files[val_region]\n",
    "\n",
    "print(f\"Training samples: {len(train_label_files)}\")\n",
    "print(f\"Validation samples: {len(val_label_files)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Data Augmentation Transformations\n",
    "# -------------------------\n",
    "class SatelliteTransform:\n",
    "    def __init__(self, augment=False):\n",
    "        self.augment = augment\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        if self.augment:\n",
    "            if random.random() > 0.5:\n",
    "                image = torch.flip(image, [2])\n",
    "                mask = torch.flip(mask, [1])\n",
    "            if random.random() > 0.5:\n",
    "                image = torch.flip(image, [1])\n",
    "                mask = torch.flip(mask, [0])\n",
    "            rot = random.choice([0, 1, 2, 3])\n",
    "            image = torch.rot90(image, rot, [1, 2])\n",
    "            mask = torch.rot90(mask, rot, [0, 1])\n",
    "        return image, mask\n",
    "\n",
    "# -------------------------\n",
    "# 4. Dataset class with improved file matching\n",
    "# -------------------------\n",
    "def extract_common_id(filename):\n",
    "    patterns = [\n",
    "        r'(Region[A-Z]+_glacier\\d+)',\n",
    "        r'([A-Za-z]+_glacier\\d+)',\n",
    "        r'([A-Za-z0-9]+_glacier\\d+)',\n",
    "        r'(glacier\\d+_[A-Za-z0-9]+)',\n",
    "        r'([A-Za-z0-9]+_\\d+)'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    filename = filename.replace('B2_', '').replace('B3_', '').replace('B4_', '').replace('B6_', '').replace('B10_', '')\n",
    "    filename = filename.replace('Y_output_resized_', '').replace('mask_', '').replace('label_', '')\n",
    "    return os.path.splitext(filename)[0]\n",
    "\n",
    "class GlacierDataset(Dataset):\n",
    "    def __init__(self, band_dirs, band_files_list, label_dir, label_files, transform=None):\n",
    "        self.band_dirs = band_dirs\n",
    "        self.band_files_list = band_files_list\n",
    "        self.label_dir = label_dir\n",
    "        self.label_files = label_files\n",
    "        self.transform = transform\n",
    "        self.file_mapping = self._create_file_mapping()\n",
    "\n",
    "    def _create_file_mapping(self):\n",
    "        mapping = {}\n",
    "        for label_file in self.label_files:\n",
    "            common_id = extract_common_id(label_file)\n",
    "            band_files = []\n",
    "            for band_files_in_dir in self.band_files_list:\n",
    "                found_file = None\n",
    "                for band_file in band_files_in_dir:\n",
    "                    band_common_id = extract_common_id(band_file)\n",
    "                    if band_common_id == common_id:\n",
    "                        found_file = band_file\n",
    "                        break\n",
    "                if found_file is None:\n",
    "                    label_base = os.path.splitext(label_file)[0]\n",
    "                    for band_file in band_files_in_dir:\n",
    "                        band_base = os.path.splitext(band_file)[0]\n",
    "                        if label_base in band_base or band_base in label_base:\n",
    "                            found_file = band_file\n",
    "                            break\n",
    "                band_files.append(found_file)\n",
    "            mapping[label_file] = band_files\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_file = self.label_files[idx]\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "        with rasterio.open(label_path) as src:\n",
    "            label_data = src.read(1)\n",
    "        label_tensor = torch.tensor(label_data, dtype=torch.float32)\n",
    "        band_files = self.file_mapping[label_file]\n",
    "        band_stack = []\n",
    "        for band_idx, band_file in enumerate(band_files):\n",
    "            if band_file is None:\n",
    "                band_data = np.zeros_like(label_data)\n",
    "            else:\n",
    "                band_path = os.path.join(self.band_dirs[band_idx], band_file)\n",
    "                with rasterio.open(band_path) as src:\n",
    "                    band_data = src.read(1)\n",
    "            band_stack.append(band_data)\n",
    "        image_tensor = torch.tensor(np.stack(band_stack, axis=0), dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image_tensor, label_tensor = self.transform(image_tensor, label_tensor)\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "train_transform = SatelliteTransform(augment=True)\n",
    "val_transform = SatelliteTransform(augment=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dataset = GlacierDataset(band_dirs, band_files_list, label_dir, train_label_files, transform=train_transform)\n",
    "    val_dataset = GlacierDataset(band_dirs, band_files_list, label_dir, val_label_files, transform=val_transform)\n",
    "\n",
    "    # Use num_workers=0 for Windows or wrap in __main__ as shown\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # 5. MCC Calculation Function\n",
    "    # -------------------------\n",
    "    def calculate_mcc(predictions, targets):\n",
    "        pred_binary = (torch.sigmoid(predictions) > 0.5).float()\n",
    "        targets = targets.unsqueeze(1) if targets.ndim == 3 else targets\n",
    "        pred_flat = pred_binary.view(-1).cpu().numpy()\n",
    "        target_flat = targets.view(-1).cpu().numpy()\n",
    "        if np.all(pred_flat == pred_flat[0]) or np.all(target_flat == target_flat[0]):\n",
    "            return 0.0\n",
    "        return matthews_corrcoef(target_flat, pred_flat)\n",
    "\n",
    "    # -------------------------\n",
    "    # 6. Define UNet model with improvements\n",
    "    # -------------------------\n",
    "    class DoubleConv(nn.Module):\n",
    "        def __init__(self, in_ch, out_ch):\n",
    "            super().__init__()\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.conv(x)\n",
    "\n",
    "    class UNet(nn.Module):\n",
    "        def __init__(self, in_ch=5, out_ch=1):\n",
    "            super().__init__()\n",
    "            self.enc1 = DoubleConv(in_ch, 64)\n",
    "            self.enc2 = DoubleConv(64, 128)\n",
    "            self.enc3 = DoubleConv(128, 256)\n",
    "            self.enc4 = DoubleConv(256, 512)\n",
    "            self.pool = nn.MaxPool2d(2)\n",
    "            self.bottleneck = DoubleConv(512, 1024)\n",
    "            self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "            self.dec4 = DoubleConv(1024, 512)\n",
    "            self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "            self.dec3 = DoubleConv(512, 256)\n",
    "            self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "            self.dec2 = DoubleConv(256, 128)\n",
    "            self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "            self.dec1 = DoubleConv(128, 64)\n",
    "            self.final = nn.Conv2d(64, out_ch, 1)\n",
    "        def forward(self, x):\n",
    "            e1 = self.enc1(x)\n",
    "            e2 = self.enc2(self.pool(e1))\n",
    "            e3 = self.enc3(self.pool(e2))\n",
    "            e4 = self.enc4(self.pool(e3))\n",
    "            b = self.bottleneck(self.pool(e4))\n",
    "            d4 = self.up4(b)\n",
    "            d4 = torch.cat([d4, e4], dim=1)\n",
    "            d4 = self.dec4(d4)\n",
    "            d3 = self.up3(d4)\n",
    "            d3 = torch.cat([d3, e3], dim=1)\n",
    "            d3 = self.dec3(d3)\n",
    "            d2 = self.up2(d3)\n",
    "            d2 = torch.cat([d2, e2], dim=1)\n",
    "            d2 = self.dec2(d2)\n",
    "            d1 = self.up1(d2)\n",
    "            d1 = torch.cat([d1, e1], dim=1)\n",
    "            d1 = self.dec1(d1)\n",
    "            return self.final(d1)\n",
    "\n",
    "    # -------------------------\n",
    "    # 7. Setup device & model\n",
    "    # -------------------------\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model = UNet(in_ch=5, out_ch=1).to(device)\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # 8. Training loop with validation and MCC-based model selection\n",
    "    # -------------------------\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "\n",
    "    num_epochs = 20\n",
    "    best_mcc = -1.0\n",
    "    best_model = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_mccs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        for images, masks in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            # Resize masks if needed\n",
    "            if masks.ndim == 3:\n",
    "                masks = masks.unsqueeze(1)\n",
    "            if masks.shape[2:] != outputs.shape[2:]:\n",
    "                masks = F.interpolate(masks.float(), size=outputs.shape[2:], mode='bilinear', align_corners=False)\n",
    "            loss = criterion(outputs, masks.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        train_loss /= train_batches\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        val_mcc_scores = []\n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\"):\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                # Resize masks if needed\n",
    "                if masks.ndim == 3:\n",
    "                    masks = masks.unsqueeze(1)\n",
    "                if masks.shape[2:] != outputs.shape[2:]:\n",
    "                    masks = F.interpolate(masks.float(), size=outputs.shape[2:], mode='bilinear', align_corners=False)\n",
    "                loss = criterion(outputs, masks.float())\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                mcc = calculate_mcc(outputs, masks)\n",
    "                val_mcc_scores.append(mcc)\n",
    "        val_loss /= val_batches\n",
    "        avg_val_mcc = np.mean(val_mcc_scores)\n",
    "        val_losses.append(val_loss)\n",
    "        val_mccs.append(avg_val_mcc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val MCC: {avg_val_mcc:.4f}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if avg_val_mcc > best_mcc:\n",
    "            best_mcc = avg_val_mcc\n",
    "            best_model = model.state_dict().copy()\n",
    "            torch.save(best_model, \"model.pth\")\n",
    "            print(f\"‚úÖ Best model updated with MCC: {best_mcc:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_val_mcc)\n",
    "\n",
    "        if epoch > 10 and avg_val_mcc < max(val_mccs[-5:]):\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    final_mcc_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            # Resize masks if needed\n",
    "            if masks.ndim == 3:\n",
    "                masks = masks.unsqueeze(1)\n",
    "            if masks.shape[2:] != outputs.shape[2:]:\n",
    "                masks = F.interpolate(masks.float(), size=outputs.shape[2:], mode='bilinear', align_corners=False)\n",
    "            mcc = calculate_mcc(outputs, masks)\n",
    "            final_mcc_scores.append(mcc)\n",
    "\n",
    "    final_mcc = np.mean(final_mcc_scores)\n",
    "    print(f\"Final MCC on unseen region: {final_mcc:.4f}\")\n",
    "    print(f\"Best model saved as 'model.pth' with MCC: {best_mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517d8f48-1af3-4134-aa0b-b79997b5a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Files extracted to: newfolder\n",
      "üìÇ Folder: newfolder\n",
      "üìÇ Folder: newfolder\\Train\n",
      "üìÇ Folder: newfolder\\Train\\Band1\n",
      "   - B2_B2_masked_02_07.tif\n",
      "   - B2_B2_masked_02_08.tif\n",
      "   - B2_B2_masked_03_07.tif\n",
      "   - B2_B2_masked_03_09.tif\n",
      "   - B2_B2_masked_03_11.tif\n",
      "   - B2_B2_masked_04_08.tif\n",
      "   - B2_B2_masked_04_09.tif\n",
      "   - B2_B2_masked_04_10.tif\n",
      "   - B2_B2_masked_05_08.tif\n",
      "   - B2_B2_masked_05_09.tif\n",
      "   - B2_B2_masked_05_10.tif\n",
      "   - B2_B2_masked_06_09.tif\n",
      "   - B2_B2_masked_06_11.tif\n",
      "   - B2_B2_masked_06_12.tif\n",
      "   - B2_B2_masked_07_10.tif\n",
      "   - B2_B2_masked_07_11.tif\n",
      "   - B2_B2_masked_07_13.tif\n",
      "   - B2_B2_masked_08_12.tif\n",
      "   - B2_B2_masked_08_13.tif\n",
      "   - B2_B2_masked_08_14.tif\n",
      "   - B2_B2_masked_09_13.tif\n",
      "   - B2_B2_masked_09_14.tif\n",
      "   - B2_B2_masked_10_12.tif\n",
      "   - B2_B2_masked_11_13.tif\n",
      "   - B2_B2_masked_12_12.tif\n",
      "üìÇ Folder: newfolder\\Train\\Band2\n",
      "   - B3_B3_masked_02_07.tif\n",
      "   - B3_B3_masked_02_08.tif\n",
      "   - B3_B3_masked_03_07.tif\n",
      "   - B3_B3_masked_03_09.tif\n",
      "   - B3_B3_masked_03_11.tif\n",
      "   - B3_B3_masked_04_08.tif\n",
      "   - B3_B3_masked_04_09.tif\n",
      "   - B3_B3_masked_04_10.tif\n",
      "   - B3_B3_masked_05_08.tif\n",
      "   - B3_B3_masked_05_09.tif\n",
      "   - B3_B3_masked_05_10.tif\n",
      "   - B3_B3_masked_06_09.tif\n",
      "   - B3_B3_masked_06_11.tif\n",
      "   - B3_B3_masked_06_12.tif\n",
      "   - B3_B3_masked_07_10.tif\n",
      "   - B3_B3_masked_07_11.tif\n",
      "   - B3_B3_masked_07_13.tif\n",
      "   - B3_B3_masked_08_12.tif\n",
      "   - B3_B3_masked_08_13.tif\n",
      "   - B3_B3_masked_08_14.tif\n",
      "   - B3_B3_masked_09_13.tif\n",
      "   - B3_B3_masked_09_14.tif\n",
      "   - B3_B3_masked_10_12.tif\n",
      "   - B3_B3_masked_11_13.tif\n",
      "   - B3_B3_masked_12_12.tif\n",
      "üìÇ Folder: newfolder\\Train\\Band3\n",
      "   - B4_B4_masked_02_07.tif\n",
      "   - B4_B4_masked_02_08.tif\n",
      "   - B4_B4_masked_03_07.tif\n",
      "   - B4_B4_masked_03_09.tif\n",
      "   - B4_B4_masked_03_11.tif\n",
      "   - B4_B4_masked_04_08.tif\n",
      "   - B4_B4_masked_04_09.tif\n",
      "   - B4_B4_masked_04_10.tif\n",
      "   - B4_B4_masked_05_08.tif\n",
      "   - B4_B4_masked_05_09.tif\n",
      "   - B4_B4_masked_05_10.tif\n",
      "   - B4_B4_masked_06_09.tif\n",
      "   - B4_B4_masked_06_11.tif\n",
      "   - B4_B4_masked_06_12.tif\n",
      "   - B4_B4_masked_07_10.tif\n",
      "   - B4_B4_masked_07_11.tif\n",
      "   - B4_B4_masked_07_13.tif\n",
      "   - B4_B4_masked_08_12.tif\n",
      "   - B4_B4_masked_08_13.tif\n",
      "   - B4_B4_masked_08_14.tif\n",
      "   - B4_B4_masked_09_13.tif\n",
      "   - B4_B4_masked_09_14.tif\n",
      "   - B4_B4_masked_10_12.tif\n",
      "   - B4_B4_masked_11_13.tif\n",
      "   - B4_B4_masked_12_12.tif\n",
      "üìÇ Folder: newfolder\\Train\\Band4\n",
      "   - B6_B6_masked_02_07.tif\n",
      "   - B6_B6_masked_02_08.tif\n",
      "   - B6_B6_masked_03_07.tif\n",
      "   - B6_B6_masked_03_09.tif\n",
      "   - B6_B6_masked_03_11.tif\n",
      "   - B6_B6_masked_04_08.tif\n",
      "   - B6_B6_masked_04_09.tif\n",
      "   - B6_B6_masked_04_10.tif\n",
      "   - B6_B6_masked_05_08.tif\n",
      "   - B6_B6_masked_05_09.tif\n",
      "   - B6_B6_masked_05_10.tif\n",
      "   - B6_B6_masked_06_09.tif\n",
      "   - B6_B6_masked_06_11.tif\n",
      "   - B6_B6_masked_06_12.tif\n",
      "   - B6_B6_masked_07_10.tif\n",
      "   - B6_B6_masked_07_11.tif\n",
      "   - B6_B6_masked_07_13.tif\n",
      "   - B6_B6_masked_08_12.tif\n",
      "   - B6_B6_masked_08_13.tif\n",
      "   - B6_B6_masked_08_14.tif\n",
      "   - B6_B6_masked_09_13.tif\n",
      "   - B6_B6_masked_09_14.tif\n",
      "   - B6_B6_masked_10_12.tif\n",
      "   - B6_B6_masked_11_13.tif\n",
      "   - B6_B6_masked_12_12.tif\n",
      "üìÇ Folder: newfolder\\Train\\Band5\n",
      "   - B10_B10_masked_02_07.tif\n",
      "   - B10_B10_masked_02_08.tif\n",
      "   - B10_B10_masked_03_07.tif\n",
      "   - B10_B10_masked_03_09.tif\n",
      "   - B10_B10_masked_03_11.tif\n",
      "   - B10_B10_masked_04_08.tif\n",
      "   - B10_B10_masked_04_09.tif\n",
      "   - B10_B10_masked_04_10.tif\n",
      "   - B10_B10_masked_05_08.tif\n",
      "   - B10_B10_masked_05_09.tif\n",
      "   - B10_B10_masked_05_10.tif\n",
      "   - B10_B10_masked_06_09.tif\n",
      "   - B10_B10_masked_06_11.tif\n",
      "   - B10_B10_masked_06_12.tif\n",
      "   - B10_B10_masked_07_10.tif\n",
      "   - B10_B10_masked_07_11.tif\n",
      "   - B10_B10_masked_07_13.tif\n",
      "   - B10_B10_masked_08_12.tif\n",
      "   - B10_B10_masked_08_13.tif\n",
      "   - B10_B10_masked_08_14.tif\n",
      "   - B10_B10_masked_09_13.tif\n",
      "   - B10_B10_masked_09_14.tif\n",
      "   - B10_B10_masked_10_12.tif\n",
      "   - B10_B10_masked_11_13.tif\n",
      "   - B10_B10_masked_12_12.tif\n",
      "üìÇ Folder: newfolder\\Train\\label\n",
      "   - Y_output_resized_02_07.tif\n",
      "   - Y_output_resized_02_08.tif\n",
      "   - Y_output_resized_03_07.tif\n",
      "   - Y_output_resized_03_09.tif\n",
      "   - Y_output_resized_03_11.tif\n",
      "   - Y_output_resized_04_08.tif\n",
      "   - Y_output_resized_04_09.tif\n",
      "   - Y_output_resized_04_10.tif\n",
      "   - Y_output_resized_05_08.tif\n",
      "   - Y_output_resized_05_09.tif\n",
      "   - Y_output_resized_05_10.tif\n",
      "   - Y_output_resized_06_09.tif\n",
      "   - Y_output_resized_06_11.tif\n",
      "   - Y_output_resized_06_12.tif\n",
      "   - Y_output_resized_07_10.tif\n",
      "   - Y_output_resized_07_11.tif\n",
      "   - Y_output_resized_07_13.tif\n",
      "   - Y_output_resized_08_12.tif\n",
      "   - Y_output_resized_08_13.tif\n",
      "   - Y_output_resized_08_14.tif\n",
      "   - Y_output_resized_09_13.tif\n",
      "   - Y_output_resized_09_14.tif\n",
      "   - Y_output_resized_10_12.tif\n",
      "   - Y_output_resized_11_13.tif\n",
      "   - Y_output_resized_12_12.tif\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"train.zip\"   \n",
    "extract_path = \"newfolder\"\n",
    "\n",
    "if os.path.exists(zip_path):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        print(\"‚úÖ Files extracted to:\", extract_path)\n",
    "\n",
    "        # Walk through extracted folder\n",
    "        for root, dirs, files in os.walk(extract_path):\n",
    "            print(\"üìÇ Folder:\", root)\n",
    "            for f in files:\n",
    "                print(\"   -\", f)\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"‚ùå Error: The file is not a valid zip or is corrupted.\")\n",
    "else:\n",
    "    print(\"‚ùå Error: File not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861a72c5-4024-4c3a-8c0f-631fd87c0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import re\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1148f270-3fe2-419b-bc90-af918234d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b22aee-381d-4066-a477-5c2be8f867d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data\n",
      "  DIR: Train\n",
      "----------------------------------------\n",
      "train_data\\Train\n",
      "  DIR: Band1\n",
      "  DIR: Band2\n",
      "  DIR: Band3\n",
      "  DIR: Band4\n",
      "  DIR: Band5\n",
      "  DIR: label\n",
      "----------------------------------------\n",
      "train_data\\Train\\Band1\n",
      "  FILE: B2_B2_masked_02_07.tif\n",
      "  FILE: B2_B2_masked_02_08.tif\n",
      "  FILE: B2_B2_masked_03_07.tif\n",
      "  FILE: B2_B2_masked_03_09.tif\n",
      "  FILE: B2_B2_masked_03_11.tif\n",
      "----------------------------------------\n",
      "train_data\\Train\\Band2\n",
      "  FILE: B3_B3_masked_02_07.tif\n",
      "  FILE: B3_B3_masked_02_08.tif\n",
      "  FILE: B3_B3_masked_03_07.tif\n",
      "  FILE: B3_B3_masked_03_09.tif\n",
      "  FILE: B3_B3_masked_03_11.tif\n",
      "----------------------------------------\n",
      "train_data\\Train\\Band3\n",
      "  FILE: B4_B4_masked_02_07.tif\n",
      "  FILE: B4_B4_masked_02_08.tif\n",
      "  FILE: B4_B4_masked_03_07.tif\n",
      "  FILE: B4_B4_masked_03_09.tif\n",
      "  FILE: B4_B4_masked_03_11.tif\n",
      "----------------------------------------\n",
      "train_data\\Train\\Band4\n",
      "  FILE: B6_B6_masked_02_07.tif\n",
      "  FILE: B6_B6_masked_02_08.tif\n",
      "  FILE: B6_B6_masked_03_07.tif\n",
      "  FILE: B6_B6_masked_03_09.tif\n",
      "  FILE: B6_B6_masked_03_11.tif\n",
      "----------------------------------------\n",
      "train_data\\Train\\Band5\n",
      "  FILE: B10_B10_masked_02_07.tif\n",
      "  FILE: B10_B10_masked_02_08.tif\n",
      "  FILE: B10_B10_masked_03_07.tif\n",
      "  FILE: B10_B10_masked_03_09.tif\n",
      "  FILE: B10_B10_masked_03_11.tif\n",
      "----------------------------------------\n",
      "train_data\\Train\\label\n",
      "  FILE: Y_output_resized_02_07.tif\n",
      "  FILE: Y_output_resized_02_08.tif\n",
      "  FILE: Y_output_resized_03_07.tif\n",
      "  FILE: Y_output_resized_03_09.tif\n",
      "  FILE: Y_output_resized_03_11.tif\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\"train_data\"):\n",
    "    print(root)\n",
    "    for d in dirs:\n",
    "        print(\"  DIR:\", d)\n",
    "    for f in files[:5]:  # only print first 5 files\n",
    "        print(\"  FILE:\", f)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41659b2-6174-4706-9ae0-a6f790dbee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è train_data already exists, skipping extraction\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'train_data\\\\Band1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m band_files_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m band_dir \u001b[38;5;129;01min\u001b[39;00m band_dirs:\n\u001b[1;32m---> 42\u001b[0m     files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(band_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.img\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[0;32m     43\u001b[0m     files\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m     44\u001b[0m     band_files_list\u001b[38;5;241m.\u001b[39mappend(files)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'train_data\\\\Band1'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# =========================================================\n",
    "# 1. Extract train.zip\n",
    "# =========================================================\n",
    "base_dir = \"train_data\"\n",
    "zip_path = \"train.zip\"\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(base_dir)\n",
    "    print(f\"‚úÖ Extracted {zip_path} to {base_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {base_dir} already exists, skipping extraction\")\n",
    "\n",
    "# =========================================================\n",
    "# 2. Define band and label directories\n",
    "# =========================================================\n",
    "band_dirs = [\n",
    "    os.path.join(base_dir, \"Band1\"),\n",
    "    os.path.join(base_dir, \"Band2\"),\n",
    "    os.path.join(base_dir, \"Band3\"),\n",
    "    os.path.join(base_dir, \"Band4\"),\n",
    "    os.path.join(base_dir, \"Band5\")\n",
    "]\n",
    "label_dir = os.path.join(base_dir, \"label\")\n",
    "\n",
    "band_files_list = []\n",
    "for band_dir in band_dirs:\n",
    "    files = [f for f in os.listdir(band_dir) if f.lower().endswith(('.tif', '.img'))]\n",
    "    files.sort()\n",
    "    band_files_list.append(files)\n",
    "\n",
    "label_files = [f for f in os.listdir(label_dir) if f.lower().endswith(('.tif', '.img'))]\n",
    "label_files.sort()\n",
    "\n",
    "# =========================================================\n",
    "# 3. Region-based split for validation\n",
    "# =========================================================\n",
    "def extract_region_id(filename):\n",
    "    patterns = [\n",
    "        r'(Region[A-Z]+)_glacier',\n",
    "        r'([A-Za-z]+)_glacier',\n",
    "        r'(region[A-Z]+)_',\n",
    "        r'([A-Za-z0-9]+)_glacier'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "region_files = {}\n",
    "for label_file in label_files:\n",
    "    region_id = extract_region_id(label_file)\n",
    "    if region_id not in region_files:\n",
    "        region_files[region_id] = []\n",
    "    region_files[region_id].append(label_file)\n",
    "\n",
    "regions = list(region_files.keys())\n",
    "print(f\"Available regions: {regions}\")\n",
    "\n",
    "if len(regions) < 2:\n",
    "    random.shuffle(label_files)\n",
    "    split_idx = int(0.8 * len(label_files))\n",
    "    train_label_files = label_files[:split_idx]\n",
    "    val_label_files = label_files[split_idx:]\n",
    "    print(\"‚ö†Ô∏è Only one region found, using random split\")\n",
    "else:\n",
    "    val_region = regions[0]\n",
    "    train_regions = regions[1:]\n",
    "    train_label_files = []\n",
    "    for region in train_regions:\n",
    "        train_label_files.extend(region_files[region])\n",
    "    val_label_files = region_files[val_region]\n",
    "\n",
    "print(f\"Training samples: {len(train_label_files)}\")\n",
    "print(f\"Validation samples: {len(val_label_files)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 4. Data Augmentation\n",
    "# =========================================================\n",
    "class SatelliteTransform:\n",
    "    def __init__(self, augment=False):\n",
    "        self.augment = augment\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        if self.augment:\n",
    "            if random.random() > 0.5:\n",
    "                image = torch.flip(image, [2])\n",
    "                mask = torch.flip(mask, [1])\n",
    "            if random.random() > 0.5:\n",
    "                image = torch.flip(image, [1])\n",
    "                mask = torch.flip(mask, [0])\n",
    "            rot = random.choice([0, 1, 2, 3])\n",
    "            image = torch.rot90(image, rot, [1, 2])\n",
    "            mask = torch.rot90(mask, rot, [0, 1])\n",
    "        return image, mask\n",
    "\n",
    "# =========================================================\n",
    "# 5. Dataset\n",
    "# =========================================================\n",
    "def extract_common_id(filename):\n",
    "    patterns = [\n",
    "        r'(Region[A-Z]+_glacier\\d+)',\n",
    "        r'([A-Za-z]+_glacier\\d+)',\n",
    "        r'([A-Za-z0-9]+_glacier\\d+)',\n",
    "        r'(glacier\\d+_[A-Za-z0-9]+)',\n",
    "        r'([A-Za-z0-9]+_\\d+)'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    filename = filename.replace('B2_', '').replace('B3_', '').replace('B4_', '').replace('B6_', '').replace('B10_', '')\n",
    "    filename = filename.replace('Y_output_resized_', '').replace('mask_', '').replace('label_', '')\n",
    "    return os.path.splitext(filename)[0]\n",
    "\n",
    "class GlacierDataset(Dataset):\n",
    "    def __init__(self, band_dirs, band_files_list, label_dir, label_files, transform=None):\n",
    "        self.band_dirs = band_dirs\n",
    "        self.band_files_list = band_files_list\n",
    "        self.label_dir = label_dir\n",
    "        self.label_files = label_files\n",
    "        self.transform = transform\n",
    "        self.file_mapping = self._create_file_mapping()\n",
    "\n",
    "    def _create_file_mapping(self):\n",
    "        mapping = {}\n",
    "        for label_file in self.label_files:\n",
    "            common_id = extract_common_id(label_file)\n",
    "            band_files = []\n",
    "            for band_files_in_dir in self.band_files_list:\n",
    "                found_file = None\n",
    "                for band_file in band_files_in_dir:\n",
    "                    band_common_id = extract_common_id(band_file)\n",
    "                    if band_common_id == common_id:\n",
    "                        found_file = band_file\n",
    "                        break\n",
    "                band_files.append(found_file)\n",
    "            mapping[label_file] = band_files\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_file = self.label_files[idx]\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "        with rasterio.open(label_path) as src:\n",
    "            label_data = src.read(1)\n",
    "        label_tensor = torch.from_numpy(label_data).float()\n",
    "        band_files = self.file_mapping[label_file]\n",
    "        band_stack = []\n",
    "        for band_idx, band_file in enumerate(band_files):\n",
    "            if band_file is None:\n",
    "                band_data = np.zeros_like(label_data)\n",
    "            else:\n",
    "                band_path = os.path.join(self.band_dirs[band_idx], band_file)\n",
    "                with rasterio.open(band_path) as src:\n",
    "                    band_data = src.read(1)\n",
    "            band_stack.append(band_data)\n",
    "        image_tensor = torch.from_numpy(np.stack(band_stack, axis=0)).float()\n",
    "        if self.transform:\n",
    "            image_tensor, label_tensor = self.transform(image_tensor, label_tensor)\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "# =========================================================\n",
    "# 6. Model (U-Net)\n",
    "# =========================================================\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=5, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.enc1 = DoubleConv(in_ch, 32)\n",
    "        self.enc2 = DoubleConv(32, 64)\n",
    "        self.enc3 = DoubleConv(64, 128)\n",
    "        self.enc4 = DoubleConv(128, 256)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = DoubleConv(256, 512)\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec4 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec3 = DoubleConv(256, 128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec2 = DoubleConv(128, 64)\n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.dec1 = DoubleConv(64, 32)\n",
    "        self.final = nn.Conv2d(32, out_ch, 1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "        d4 = self.up4(b)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        return self.final(d1)\n",
    "\n",
    "# =========================================================\n",
    "# 7. Training Setup\n",
    "# =========================================================\n",
    "train_transform = SatelliteTransform(augment=True)\n",
    "val_transform = SatelliteTransform(augment=False)\n",
    "\n",
    "train_dataset = GlacierDataset(band_dirs, band_files_list, label_dir, train_label_files, transform=train_transform)\n",
    "val_dataset = GlacierDataset(band_dirs, band_files_list, label_dir, val_label_files, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "def calculate_mcc(predictions, targets):\n",
    "    pred_binary = (torch.sigmoid(predictions) > 0.5).float()\n",
    "    targets = targets.unsqueeze(1) if targets.ndim == 3 else targets\n",
    "    pred_flat = pred_binary.view(-1).cpu().numpy()\n",
    "    target_flat = targets.view(-1).cpu().numpy()\n",
    "    if np.all(pred_flat == pred_flat[0]) or np.all(target_flat == target_flat[0]):\n",
    "        return 0.0\n",
    "    return matthews_corrcoef(target_flat, pred_flat)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(in_ch=5, out_ch=1).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=3, factor=0.5)\n",
    "\n",
    "# =========================================================\n",
    "# 8. Training Loop\n",
    "# =========================================================\n",
    "num_epochs = 20\n",
    "best_mcc = -1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        if masks.ndim == 3:\n",
    "            masks = masks.unsqueeze(1)\n",
    "        if masks.shape[2:] != outputs.shape[2:]:\n",
    "            masks = F.interpolate(masks.float(), size=outputs.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        loss = criterion(outputs, masks.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mccs = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            if masks.ndim == 3:\n",
    "                masks = masks.unsqueeze(1)\n",
    "            if masks.shape[2:] != outputs.shape[2:]:\n",
    "                masks = F.interpolate(masks.float(), size=outputs.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "            loss = criterion(outputs, masks.float())\n",
    "            val_loss += loss.item()\n",
    "            val_mccs.append(calculate_mcc(outputs, masks))\n",
    "    val_loss /= len(val_loader)\n",
    "    avg_val_mcc = np.mean(val_mccs)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}, Val MCC {avg_val_mcc:.4f}\")\n",
    "\n",
    "    if avg_val_mcc > best_mcc:\n",
    "        best_mcc = avg_val_mcc\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "        print(f\"‚úÖ Best model updated (MCC={best_mcc:.4f})\")\n",
    "\n",
    "    scheduler.step(avg_val_mcc)\n",
    "\n",
    "print(f\"Final Best MCC: {best_mcc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093f4de6-f0cd-4974-9d2f-88d450f3f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of train_data:\n",
      "['Train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Contents of train_data:\")\n",
    "print(os.listdir(\"train_data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0241b50-5203-4761-bfc3-ba285706dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in train_data: ['Train']\n",
      "‚úÖ Dataset root set to: train_data\\Train\n",
      "Band dirs: ['train_data\\\\Train\\\\Band1', 'train_data\\\\Train\\\\Band2', 'train_data\\\\Train\\\\Band3', 'train_data\\\\Train\\\\Band4', 'train_data\\\\Train\\\\Band5']\n",
      "Label dir: train_data\\Train\\label\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"train_data\"\n",
    "\n",
    "# Find the first directory inside train_data that contains Band1\n",
    "possible_subdirs = os.listdir(base_dir)\n",
    "print(\"Found in train_data:\", possible_subdirs)\n",
    "\n",
    "# If train_data directly has Band1, keep it\n",
    "if \"Band1\" in possible_subdirs:\n",
    "    dataset_root = base_dir\n",
    "else:\n",
    "    # Otherwise, enter the first subdirectory\n",
    "    dataset_root = os.path.join(base_dir, possible_subdirs[0])\n",
    "\n",
    "print(\"‚úÖ Dataset root set to:\", dataset_root)\n",
    "\n",
    "# Now define band and label dirs\n",
    "band_dirs = [os.path.join(dataset_root, f\"Band{i}\") for i in range(1, 6)]\n",
    "label_dir = os.path.join(dataset_root, \"label\")\n",
    "\n",
    "print(\"Band dirs:\", band_dirs)\n",
    "print(\"Label dir:\", label_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ffa3b2-cdaa-4b31-baab-b31e9659a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è train_data already exists, skipping extraction\n",
      "Band dirs: ['train_data\\\\Train\\\\Band1', 'train_data\\\\Train\\\\Band2', 'train_data\\\\Train\\\\Band3', 'train_data\\\\Train\\\\Band4', 'train_data\\\\Train\\\\Band5']\n",
      "Label dir: train_data\\Train\\label\n",
      "Available regions: ['Y']\n",
      "‚ö†Ô∏è Only one region found, using random split\n",
      "Training samples: 20\n",
      "Validation samples: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:32<00:00, 15.21s/it]\n",
      "Epoch 1/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss -1.3134, Val Loss -2.5509, Val MCC 0.0000\n",
      "‚úÖ Best model updated (MCC=0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:30<00:00, 15.01s/it]\n",
      "Epoch 2/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss -17.9297, Val Loss -17.0819, Val MCC 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:31<00:00, 15.19s/it]\n",
      "Epoch 3/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss -28.1828, Val Loss -38.4345, Val MCC 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:29<00:00, 14.91s/it]\n",
      "Epoch 4/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss -37.6717, Val Loss -56.0392, Val MCC 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:28<00:00, 14.81s/it]\n",
      "Epoch 5/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss -47.5799, Val Loss -69.1643, Val MCC 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:24<00:00, 14.48s/it]\n",
      "Epoch 6/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss -56.2905, Val Loss -73.2359, Val MCC 0.0024\n",
      "‚úÖ Best model updated (MCC=0.0024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:24<00:00, 14.44s/it]\n",
      "Epoch 7/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss -62.5147, Val Loss -74.8486, Val MCC 0.0062\n",
      "‚úÖ Best model updated (MCC=0.0062)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:25<00:00, 14.56s/it]\n",
      "Epoch 8/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss -68.7261, Val Loss -81.5348, Val MCC 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:28<00:00, 14.80s/it]\n",
      "Epoch 9/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss -75.3968, Val Loss -89.1200, Val MCC 0.0044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:38<00:00, 15.86s/it]\n",
      "Epoch 10/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss -83.4486, Val Loss -99.1578, Val MCC 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:35<00:00, 15.54s/it]\n",
      "Epoch 11/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss -90.9042, Val Loss -111.7120, Val MCC 0.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:26<00:00, 14.60s/it]\n",
      "Epoch 12/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:13<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss -96.4960, Val Loss -116.5011, Val MCC 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:36<00:00, 15.64s/it]\n",
      "Epoch 13/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss -100.0061, Val Loss -118.6824, Val MCC 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:35<00:00, 15.56s/it]\n",
      "Epoch 14/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss -103.5234, Val Loss -120.6295, Val MCC 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:36<00:00, 15.62s/it]\n",
      "Epoch 15/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:11<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss -106.9715, Val Loss -123.2523, Val MCC 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:32<00:00, 15.25s/it]\n",
      "Epoch 16/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss -109.5510, Val Loss -124.7446, Val MCC 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:35<00:00, 15.53s/it]\n",
      "Epoch 17/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss -111.2842, Val Loss -125.4877, Val MCC 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:47<00:00, 16.71s/it]\n",
      "Epoch 18/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:14<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss -112.9333, Val Loss -126.2410, Val MCC 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [03:08<00:00, 18.88s/it]\n",
      "Epoch 19/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:18<00:00,  6.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss -114.7032, Val Loss -128.0560, Val MCC 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [03:08<00:00, 18.83s/it]\n",
      "Epoch 20/20 - Val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:12<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss -116.0190, Val Loss -128.7533, Val MCC 0.0000\n",
      "Final Best MCC: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# =========================================================\n",
    "# 1. Extract train.zip\n",
    "# =========================================================\n",
    "base_dir = \"train_data\"\n",
    "zip_path = \"train.zip\"\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(base_dir)\n",
    "    print(f\"‚úÖ Extracted {zip_path} to {base_dir}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {base_dir} already exists, skipping extraction\")\n",
    "\n",
    "# =========================================================\n",
    "# 2. Correct dataset root (inside 'Train' folder)\n",
    "# =========================================================\n",
    "dataset_root = os.path.join(base_dir, \"Train\")\n",
    "\n",
    "band_dirs = [os.path.join(dataset_root, f\"Band{i}\") for i in range(1, 6)]\n",
    "label_dir = os.path.join(dataset_root, \"label\")\n",
    "\n",
    "print(\"Band dirs:\", band_dirs)\n",
    "print(\"Label dir:\", label_dir)\n",
    "\n",
    "# =========================================================\n",
    "# 3. Collect band and label files\n",
    "# =========================================================\n",
    "band_files_list = []\n",
    "for band_dir in band_dirs:\n",
    "    files = [f for f in os.listdir(band_dir) if f.lower().endswith(('.tif', '.img'))]\n",
    "    files.sort()\n",
    "    band_files_list.append(files)\n",
    "\n",
    "label_files = [f for f in os.listdir(label_dir) if f.lower().endswith(('.tif', '.img'))]\n",
    "label_files.sort()\n",
    "\n",
    "# =========================================================\n",
    "# 4. Region-based split for validation\n",
    "# =========================================================\n",
    "def extract_region_id(filename):\n",
    "    patterns = [\n",
    "        r'(Region[A-Z]+)_glacier',\n",
    "        r'([A-Za-z]+)_glacier',\n",
    "        r'(region[A-Z]+)_',\n",
    "        r'([A-Za-z0-9]+)_glacier'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "region_files = {}\n",
    "for label_file in label_files:\n",
    "    region_id = extract_region_id(label_file)\n",
    "    if region_id not in region_files:\n",
    "        region_files[region_id] = []\n",
    "    region_files[region_id].append(label_file)\n",
    "\n",
    "regions = list(region_files.keys())\n",
    "print(f\"Available regions: {regions}\")\n",
    "\n",
    "if len(regions) < 2:\n",
    "    random.shuffle(label_files)\n",
    "    split_idx = int(0.8 * len(label_files))\n",
    "    train_label_files = label_files[:split_idx]\n",
    "    val_label_files = label_files[split_idx:]\n",
    "    print(\"‚ö†Ô∏è Only one region found, using random split\")\n",
    "else:\n",
    "    val_region = regions[0]\n",
    "    train_regions = regions[1:]\n",
    "    train_label_files = []\n",
    "    for region in train_regions:\n",
    "        train_label_files.extend(region_files[region])\n",
    "    val_label_files = region_files[val_region]\n",
    "\n",
    "print(f\"Training samples: {len(train_label_files)}\")\n",
    "print(f\"Validation samples: {len(val_label_files)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 5. Data Augmentation\n",
    "# =========================================================\n",
    "class SatelliteTransform:\n",
    "    def __init__(self, augment=False):\n",
    "        self.augment = augment\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        if self.augment:\n",
    "            if random.random() > 0.5:\n",
    "                image = torch.flip(image, [2])\n",
    "                mask = torch.flip(mask, [1])\n",
    "            if random.random() > 0.5:\n",
    "                image = torch.flip(image, [1])\n",
    "                mask = torch.flip(mask, [0])\n",
    "            rot = random.choice([0, 1, 2, 3])\n",
    "            image = torch.rot90(image, rot, [1, 2])\n",
    "            mask = torch.rot90(mask, rot, [0, 1])\n",
    "        return image, mask\n",
    "\n",
    "# =========================================================\n",
    "# 6. Dataset\n",
    "# =========================================================\n",
    "def extract_common_id(filename):\n",
    "    patterns = [\n",
    "        r'(Region[A-Z]+_glacier\\d+)',\n",
    "        r'([A-Za-z]+_glacier\\d+)',\n",
    "        r'([A-Za-z0-9]+_glacier\\d+)',\n",
    "        r'(glacier\\d+_[A-Za-z0-9]+)',\n",
    "        r'([A-Za-z0-9]+_\\d+)'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    filename = filename.replace('B2_', '').replace('B3_', '').replace('B4_', '').replace('B6_', '').replace('B10_', '')\n",
    "    filename = filename.replace('Y_output_resized_', '').replace('mask_', '').replace('label_', '')\n",
    "    return os.path.splitext(filename)[0]\n",
    "\n",
    "class GlacierDataset(Dataset):\n",
    "    def __init__(self, band_dirs, band_files_list, label_dir, label_files, transform=None):\n",
    "        self.band_dirs = band_dirs\n",
    "        self.band_files_list = band_files_list\n",
    "        self.label_dir = label_dir\n",
    "        self.label_files = label_files\n",
    "        self.transform = transform\n",
    "        self.file_mapping = self._create_file_mapping()\n",
    "\n",
    "    def _create_file_mapping(self):\n",
    "        mapping = {}\n",
    "        for label_file in self.label_files:\n",
    "            common_id = extract_common_id(label_file)\n",
    "            band_files = []\n",
    "            for band_files_in_dir in self.band_files_list:\n",
    "                found_file = None\n",
    "                for band_file in band_files_in_dir:\n",
    "                    band_common_id = extract_common_id(band_file)\n",
    "                    if band_common_id == common_id:\n",
    "                        found_file = band_file\n",
    "                        break\n",
    "                band_files.append(found_file)\n",
    "            mapping[label_file] = band_files\n",
    "        return mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_file = self.label_files[idx]\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "        with rasterio.open(label_path) as src:\n",
    "            label_data = src.read(1)\n",
    "        label_tensor = torch.from_numpy(label_data).float()\n",
    "        band_files = self.file_mapping[label_file]\n",
    "        band_stack = []\n",
    "        for band_idx, band_file in enumerate(band_files):\n",
    "            if band_file is None:\n",
    "                band_data = np.zeros_like(label_data)\n",
    "            else:\n",
    "                band_path = os.path.join(self.band_dirs[band_idx], band_file)\n",
    "                with rasterio.open(band_path) as src:\n",
    "                    band_data = src.read(1)\n",
    "            band_stack.append(band_data)\n",
    "        image_tensor = torch.from_numpy(np.stack(band_stack, axis=0)).float()\n",
    "        if self.transform:\n",
    "            image_tensor, label_tensor = self.transform(image_tensor, label_tensor)\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "# =========================================================\n",
    "# 7. Model (U-Net)\n",
    "# =========================================================\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_ch=5, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.enc1 = DoubleConv(in_ch, 32)\n",
    "        self.enc2 = DoubleConv(32, 64)\n",
    "        self.enc3 = DoubleConv(64, 128)\n",
    "        self.enc4 = DoubleConv(128, 256)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bottleneck = DoubleConv(256, 512)\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec4 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec3 = DoubleConv(256, 128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec2 = DoubleConv(128, 64)\n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.dec1 = DoubleConv(64, 32)\n",
    "        self.final = nn.Conv2d(32, out_ch, 1)\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "        d4 = self.up4(b)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        return self.final(d1)\n",
    "\n",
    "# =========================================================\n",
    "# 8. Training Setup\n",
    "# =========================================================\n",
    "train_transform = SatelliteTransform(augment=True)\n",
    "val_transform = SatelliteTransform(augment=False)\n",
    "\n",
    "train_dataset = GlacierDataset(band_dirs, band_files_list, label_dir, train_label_files, transform=train_transform)\n",
    "val_dataset = GlacierDataset(band_dirs, band_files_list, label_dir, val_label_files, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "def calculate_mcc(predictions, targets):\n",
    "    pred_binary = (torch.sigmoid(predictions) > 0.5).float()\n",
    "    targets = targets.unsqueeze(1) if targets.ndim == 3 else targets\n",
    "    pred_flat = pred_binary.view(-1).cpu().numpy()\n",
    "    target_flat = targets.view(-1).cpu().numpy()\n",
    "    if np.all(pred_flat == pred_flat[0]) or np.all(target_flat == target_flat[0]):\n",
    "        return 0.0\n",
    "    return matthews_corrcoef(target_flat, pred_flat)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(in_ch=5, out_ch=1).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=3, factor=0.5)\n",
    "\n",
    "# =========================================================\n",
    "# 9. Training Loop\n",
    "# =========================================================\n",
    "num_epochs = 20\n",
    "best_mcc = -1.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        if masks.ndim == 3:\n",
    "            masks = masks.unsqueeze(1)\n",
    "        if masks.shape[2:] != outputs.shape[2:]:\n",
    "            masks = F.interpolate(masks.float(), size=outputs.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        loss = criterion(outputs, masks.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mccs = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Val\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            if masks.ndim == 3:\n",
    "                masks = masks.unsqueeze(1)\n",
    "            if masks.shape[2:] != outputs.shape[2:]:\n",
    "                masks = F.interpolate(masks.float(), size=outputs.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "            loss = criterion(outputs, masks.float())\n",
    "            val_loss += loss.item()\n",
    "            val_mccs.append(calculate_mcc(outputs, masks))\n",
    "    val_loss /= len(val_loader)\n",
    "    avg_val_mcc = np.mean(val_mccs)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}, Val MCC {avg_val_mcc:.4f}\")\n",
    "\n",
    "    if avg_val_mcc > best_mcc:\n",
    "        best_mcc = avg_val_mcc\n",
    "        torch.save(model.state_dict(), \"model.pth\")\n",
    "        print(f\"‚úÖ Best model updated (MCC={best_mcc:.4f})\")\n",
    "\n",
    "    scheduler.step(avg_val_mcc)\n",
    "\n",
    "print(f\"Final Best MCC: {best_mcc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dee8e5-c59e-44ec-9b52-cf61cc764a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
